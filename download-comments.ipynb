{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "## Download Comments Source CSV\n",
    "1. Download the fill list of available comments from [https://www.regulations.gov/docketBrowser?rpp=25&so=DESC&sb=commentDueDate&po=0&dct=PS&D=DOI-2017-0002]\n",
    "2. Open DOCKET_DOI-2017-0002.csv and delete lines 1 - 5, e.g. everything before \"Document Title,Document Type,Attachment Count...\" \n",
    "3. Copy file (DOCKET_DOI-2017-0002.csv) to working directory\n",
    "\n",
    "## Activate Source Environment\n",
    "```bash\n",
    "source activate benm\n",
    "jupyter notebook\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start here!\n",
    "\n",
    "# to do: download attachments\n",
    "\n",
    "# import db_settings\n",
    "import psycopg2\n",
    "import os, errno, csv\n",
    "\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from dateutil.parser import parse\n",
    "from datetime import date\n",
    "\n",
    "ignore_list = []\n",
    "\n",
    "TIMEOUT = 15 # WebDriver timeout (page load, etc.), in seconds\n",
    "\n",
    "def benm_driver():\n",
    "    fp = webdriver.FirefoxProfile()\n",
    "    fp.set_preference(\"http.response.timeout\", TIMEOUT)\n",
    "    fp.set_preference(\"dom.max_script_run_time\", TIMEOUT)\n",
    "    driver = webdriver.Firefox(firefox_profile=fp)\n",
    "    driver.implicitly_wait(TIMEOUT) # seconds\n",
    "    return driver\n",
    "\n",
    "def get_comments(comments):\n",
    "    conn = psycopg2.connect(\"dbname=benm user=postgres\")\n",
    "    conn.set_session(autocommit=True)\n",
    "    cur = conn.cursor()\n",
    "    query = 'INSERT INTO comments (document_id, tracking_number, date_posted, comment, has_attachments, retrieved) VALUES (%(document_id)s, %(tracking_number)s, %(date_posted)s, %(comment_text)s, %(has_attachments)s, now());'\n",
    "    ignore_query = \"INSERT INTO ignore_list (document_id, reason) VALUES (%s, %s);\"\n",
    "\n",
    "    driver = benm_driver()\n",
    "    \n",
    "    for comment in comments:\n",
    "        comment_values = get_comment(driver, comment['Document Detail'], comment['Document ID'])\n",
    "        if comment_values:\n",
    "            try:\n",
    "                cur.execute(query, comment_values)\n",
    "            except Exception as ex:\n",
    "                if ex.pgcode == '23505': # unique constraint violated\n",
    "                    cur.execute(ignore_query, (comment['Document ID'], 'Duplicate Tracking Number'))\n",
    "                    print('Duplicate tracking number found for comment %s.' % comment['Document ID'])\n",
    "                else:\n",
    "                    print(ex)\n",
    "                    print('consider adding %s to ignore list' % comment['Document ID'])\n",
    "                    raise\n",
    "        else:\n",
    "            print(\"Error retrieving document %s.\" % comment['Document ID'])\n",
    "            driver.quit()\n",
    "            driver = benm_driver()\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    # conn.commit() # Make the changes to the database persistent. Not used if autocommit=True\n",
    "    conn.close()\n",
    "    \n",
    "def get_comment(driver, url, source_document_id):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        element = WebDriverWait(driver, TIMEOUT).until(\n",
    "            EC.title_is(\"Regulations.gov - Comment\")\n",
    "        )\n",
    "\n",
    "        element = driver.find_element(By.XPATH, '/html/body/div[3]/div[2]/div[2]/div[3]/div/table/tbody/tr/td[3]/div/div/div[2]/div[1]/div[1]/span[2]')\n",
    "        document_id = element.text\n",
    "        assert document_id == source_document_id # make sure the page matches\n",
    "        element = driver.find_element(By.XPATH, '/html/body/div[3]/div[2]/div[2]/div[3]/div/table/tbody/tr/td[3]/div/div/div[2]/div[1]/div[2]/span[2]')\n",
    "        tracking_number = element.text\n",
    "        element = driver.find_element(By.XPATH, '/html/body/div[3]/div[2]/div[2]/div[3]/div/table/tbody/tr/td[3]/div/div/div[2]/div[4]/div/div/span[2]')\n",
    "        d = parse(element.text)\n",
    "        date_posted = date(d.year, d.month, d.day)\n",
    "        element = driver.find_element(By.XPATH, '/html/body/div[3]/div[2]/div[2]/div[3]/div/table/tbody/tr/td[1]/div/div[3]/div[1]/div/div[2]')\n",
    "        comment_text = element.text\n",
    "        try:\n",
    "            element = driver.find_element(By.XPATH, '/html/body/div[3]/div[2]/div[2]/div[3]/div/table/tbody/tr/td[1]/div/div[3]/div[2]/div[1]/h2/span')\n",
    "            has_attachments = (element.text == \"Attachments\")\n",
    "        except NoSuchElementException:\n",
    "            has_attachments = False\n",
    "\n",
    "        result = { \n",
    "            'document_id': document_id,\n",
    "            'tracking_number': tracking_number,\n",
    "            'date_posted': date_posted,\n",
    "            'comment_text': comment_text,\n",
    "            'has_attachments': has_attachments\n",
    "        }\n",
    "\n",
    "        return result\n",
    "    \n",
    "    except TimeoutException as ex:\n",
    "        return False\n",
    "    except NoSuchElementException as ex:\n",
    "        return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read list of comments to download from CSV\n",
    "with open('DOCKET_DOI-2017-0002.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    comments = [row for row in reader]\n",
    "\n",
    "# ignore withdrawn comments and others of non \"public submission\" type\n",
    "comments = [c for c in comments if c['Document Type'] == 'PUBLIC SUBMISSIONS']\n",
    "  \n",
    "comments = sorted(comments, key=lambda k: int(k['Document ID'][14:])) # sort in ascending order\n",
    "\n",
    "with psycopg2.connect(\"dbname=benm user=postgres\") as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        try:\n",
    "            cur.execute('SELECT document_id FROM comments;')\n",
    "            downloaded_comments = set([c[0] for c in cur.fetchall()])\n",
    "            \n",
    "            cur.execute('SELECT document_id FROM ignore_list;')\n",
    "            ignore_list = set([c[0] for c in cur.fetchall()])\n",
    "        except psycopg2.Error as e:\n",
    "            print (query)\n",
    "            print (e.pgerror)\n",
    "conn.close()\n",
    "\n",
    "print('comments in db: ' + str(len(downloaded_comments)))\n",
    "print('ignore list: ' + str(len(ignore_list)))\n",
    "\n",
    "comments = [c for c in comments if not c['Document ID'] in downloaded_comments]\n",
    "comments = [c for c in comments if not c['Document ID'] in ignore_list]\n",
    "\n",
    "print('remaining comments: ' + str(len(comments)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spawn 8 workers to scrape batch_size comments each\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "workers = 8\n",
    "batch_size = int(len(comments) / workers)\n",
    "\n",
    "processes = []\n",
    "\n",
    "for i in range(workers):\n",
    "    processes.append(Process(target=get_comments, args=(comments[i*batch_size:((i+1)*batch_size)-1],)))\n",
    "    processes[i].start()\n",
    "\n",
    "for i in range(workers):\n",
    "    processes[i].join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pandas.io.sql as psql\n",
    "\n",
    "# write comments to CSV\n",
    "conn = psycopg2.connect(\"dbname=benm user=postgres\")\n",
    "df = psql.read_sql(\"SELECT document_id, tracking_number, date_posted, retrieved, has_attachments, comment FROM comments;\", conn)\n",
    "df[['document_url']] = 'https://www.regulations.gov/document?D=' + df[['document_id']]\n",
    "\n",
    "# sort rows by document id, then drop the internal database id column\n",
    "df['id'] = df['document_id'].str[14:]\n",
    "df = df.sort_values(by='id')\n",
    "df = df.drop('id', 1) # 1 is the axis number, 0 for rows, 1 for columns\n",
    "df.to_csv('dataset/comments.csv', index=False)\n",
    "\n",
    "# write ignore list to CSV\n",
    "df = psql.read_sql(\"SELECT * FROM ignore_list;\", conn)\n",
    "df[['document_url']] = 'https://www.regulations.gov/document?D=' + df[['document_id']]\n",
    "\n",
    "# sort rows by document id, then drop the internal database id column\n",
    "df['id'] = df['document_id'].str[14:]\n",
    "df = df.sort_values(by='id')\n",
    "df = df.drop('id', 1) # 1 is the axis number, 0 for rows, 1 for columns\n",
    "df.to_csv('dataset/ignore_list.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to do - automate packaging?\n",
    "# to package - create xlsx version, save as zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
