{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Regulations.gov hosts the publicly submitted comments that are part of Sec. Zinke's monuments review. You can browse the comments there, and download a CSV that lists the comment numbers. This project scrapes these comments into a database (see: [setup.ipynb]) using Selenium Webdriver.\n",
    "\n",
    "Unfortuantely, the CSV download only lists the most recent 103,000 comments. Regulations.gov confirms it's a limitation on their end. With over 390,000 comments available, a new method is needed. This notebook takes advantage of the fact that the posted comments are numbered sequentially.\n",
    "\n",
    "# Instructions\n",
    "\n",
    "\n",
    "## Activate Source Environment\n",
    "```bash\n",
    "source activate benm\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "## Download Comments by Number\n",
    "1. The full list of comments is provided at [https://www.regulations.gov/docketBrowser?rpp=25&so=DESC&sb=commentDueDate&po=0&dct=PS&D=DOI-2017-0002]\n",
    "2. Open the URL above and update MAX_COMMENT_NUMBER below with the the number of results listed (e.g. 390,376 Results) \n",
    "3. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start here!\n",
    "\n",
    "# Update the constant below with the number of results listed on regulations.gov\n",
    "\n",
    "MAX_COMMENT_NUMBER = 390376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some helper functions\n",
    "\n",
    "# to do: download attachments\n",
    "\n",
    "import psycopg2\n",
    "import os, errno, csv\n",
    "\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from dateutil.parser import parse\n",
    "from datetime import date\n",
    "\n",
    "ignore_list = []\n",
    "\n",
    "TIMEOUT = 15 # WebDriver timeout (page load, etc.), in seconds\n",
    "\n",
    "def benm_driver():\n",
    "    fp = webdriver.FirefoxProfile()\n",
    "    fp.set_preference(\"http.response.timeout\", TIMEOUT)\n",
    "    fp.set_preference(\"dom.max_script_run_time\", TIMEOUT)\n",
    "    driver = webdriver.Firefox(firefox_profile=fp)\n",
    "    driver.implicitly_wait(TIMEOUT) # seconds\n",
    "    return driver\n",
    "\n",
    "def get_comments(comments):\n",
    "    conn = psycopg2.connect(\"dbname=benm user=postgres\")\n",
    "    conn.set_session(autocommit=True)\n",
    "    cur = conn.cursor()\n",
    "    query = 'INSERT INTO comments (document_id, tracking_number, date_posted, comment, has_attachments, retrieved) VALUES (%(document_id)s, %(tracking_number)s, %(date_posted)s, %(comment_text)s, %(has_attachments)s, now());'\n",
    "    ignore_query = \"INSERT INTO ignore_list (document_id, reason) VALUES (%s, %s);\"\n",
    "\n",
    "    driver = benm_driver()\n",
    "    \n",
    "    for comment in comments:\n",
    "        document_id = 'DOI-2017-0002-' + '{0:04d}'.format(comment) # pad numbers less than 1000 with leading zeros\n",
    "        comment_values = get_comment(driver, document_id)\n",
    "        if comment_values:\n",
    "            try:\n",
    "                cur.execute(query, comment_values)\n",
    "            except Exception as ex:\n",
    "                if ex.pgcode == '23505': # unique constraint violated\n",
    "                    cur.execute(ignore_query, (document_id, 'Duplicate Tracking Number'))\n",
    "                    print('Duplicate tracking number found for comment %s' % document_id)\n",
    "                else:\n",
    "                    print(ex)\n",
    "                    print('consider adding %s to ignore list' % document_id)\n",
    "                    raise\n",
    "        else:\n",
    "            print(\"Error retrieving document %s.\" % document_id)\n",
    "            cur.execute(ignore_query, (document_id, 'unknown error'))\n",
    "            driver.quit()\n",
    "            driver = benm_driver()\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    # conn.commit() # Make the changes to the database persistent. Not used if autocommit=True\n",
    "    conn.close()\n",
    "    \n",
    "def get_comment(driver, document_id):\n",
    "    try:\n",
    "        url = 'https://www.regulations.gov/document?D=' + document_id\n",
    "        driver.get(url)\n",
    "        element = WebDriverWait(driver, TIMEOUT).until(\n",
    "            EC.title_is(\"Regulations.gov - Comment\")\n",
    "        )\n",
    "\n",
    "        element = driver.find_element(By.XPATH, '/html/body/div[3]/div[2]/div[2]/div[3]/div/table/tbody/tr/td[3]/div/div/div[2]/div[1]/div[1]/span[2]')\n",
    "        source_document_id = element.text\n",
    "        assert document_id == source_document_id # make sure the page matches\n",
    "        element = driver.find_element(By.XPATH, '/html/body/div[3]/div[2]/div[2]/div[3]/div/table/tbody/tr/td[3]/div/div/div[2]/div[1]/div[2]/span[2]')\n",
    "        tracking_number = element.text\n",
    "        element = driver.find_element(By.XPATH, '/html/body/div[3]/div[2]/div[2]/div[3]/div/table/tbody/tr/td[3]/div/div/div[2]/div[4]/div/div/span[2]')\n",
    "        d = parse(element.text)\n",
    "        date_posted = date(d.year, d.month, d.day)\n",
    "        element = driver.find_element(By.XPATH, '/html/body/div[3]/div[2]/div[2]/div[3]/div/table/tbody/tr/td[1]/div/div[3]/div[1]/div/div[2]')\n",
    "        comment_text = element.text\n",
    "        try:\n",
    "            element = driver.find_element(By.XPATH, '/html/body/div[3]/div[2]/div[2]/div[3]/div/table/tbody/tr/td[1]/div/div[3]/div[2]/div[1]/h2/span')\n",
    "            has_attachments = (element.text == \"Attachments\")\n",
    "        except NoSuchElementException:\n",
    "            has_attachments = False\n",
    "\n",
    "        result = { \n",
    "            'document_id': document_id,\n",
    "            'tracking_number': tracking_number,\n",
    "            'date_posted': date_posted,\n",
    "            'comment_text': comment_text,\n",
    "            'has_attachments': has_attachments\n",
    "        }\n",
    "\n",
    "        return result\n",
    "    \n",
    "    except TimeoutException as ex:\n",
    "        return False\n",
    "    except NoSuchElementException as ex:\n",
    "        return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = range(1000, MAX_COMMENT_NUMBER)\n",
    "\n",
    "# get the set of IDs of comments we've already downloaded, and the ignore list\n",
    "with psycopg2.connect(\"dbname=benm user=postgres\") as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        try:\n",
    "            cur.execute('SELECT substring(document_id from 15)::int FROM comments ORDER BY substring(document_id from 15)::int;')\n",
    "            downloaded_comments = set([c[0] for c in cur.fetchall()])\n",
    "            \n",
    "            cur.execute('SELECT substring(document_id from 15)::int FROM ignore_list;')\n",
    "            ignore_list = set([c[0] for c in cur.fetchall()])\n",
    "        except psycopg2.Error as e:\n",
    "            print (query)\n",
    "            print (e.pgerror)\n",
    "conn.close()\n",
    "\n",
    "# intersect the range of IDs and the set of downloaded/ignore comments, and \n",
    "# take the difference as our working list\n",
    "dl_list = set(ids).difference(downloaded_comments.union(ignore_list))\n",
    "dl_list = list(dl_list)\n",
    "print('comments in db: ' + str(len(downloaded_comments)))\n",
    "print('ignore list: ' + str(len(ignore_list)))\n",
    "print('to download: ' + str(len(dl_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spawn workers to scrape batch_size comments each\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "workers = 4 # 16\n",
    "batch_size = int(len(dl_list) / workers)\n",
    "\n",
    "processes = []\n",
    "\n",
    "for i in range(workers):\n",
    "    processes.append(Process(target=get_comments, args=(dl_list[i*batch_size:((i+1)*batch_size)-1],)))\n",
    "    processes[i].start()\n",
    "\n",
    "for i in range(workers):\n",
    "    processes[i].join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pandas.io.sql as psql\n",
    "\n",
    "# write comments to CSV\n",
    "conn = psycopg2.connect(\"dbname=benm user=postgres\")\n",
    "df = psql.read_sql(\"SELECT document_id, tracking_number, date_posted, retrieved, has_attachments, comment FROM comments;\", conn)\n",
    "df[['document_url']] = 'https://www.regulations.gov/document?D=' + df[['document_id']]\n",
    "\n",
    "# sort rows by document id, then drop the internal database id column\n",
    "df['id'] = df['document_id'].str[14:]\n",
    "df = df.sort_values(by='id')\n",
    "df = df.drop('id', 1) # 1 is the axis number, 0 for rows, 1 for columns\n",
    "df.to_csv('dataset/comments.csv', index=False)\n",
    "\n",
    "# write ignore list to CSV\n",
    "df = psql.read_sql(\"SELECT * FROM ignore_list;\", conn)\n",
    "df[['document_url']] = 'https://www.regulations.gov/document?D=' + df[['document_id']]\n",
    "\n",
    "# sort rows by document id, then drop the internal database id column\n",
    "df['id'] = df['document_id'].str[14:]\n",
    "df = df.sort_values(by='id')\n",
    "df = df.drop('id', 1) # 1 is the axis number, 0 for rows, 1 for columns\n",
    "df.to_csv('dataset/ignore_list.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to do - automate packaging?\n",
    "# to package - create xlsx version, save as zip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
